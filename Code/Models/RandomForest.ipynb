{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hour</th>\n",
       "      <th>station_id</th>\n",
       "      <th>Start_Count</th>\n",
       "      <th>Stop_Count</th>\n",
       "      <th>Net_Flow</th>\n",
       "      <th>Split_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>feels_like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-21 22:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-28 21:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-03 16:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-02-04 16:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-02-07 15:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 Hour  station_id  Start_Count  Stop_Count  \\\n",
       "0           0  2021-01-21 22:00:00           1            1           0   \n",
       "1           1  2021-01-28 21:00:00           1            1           0   \n",
       "2           2  2021-02-03 16:00:00           1            1           0   \n",
       "3           3  2021-02-04 16:00:00           1            1           0   \n",
       "4           4  2021-02-07 15:00:00           1            1           0   \n",
       "\n",
       "   Net_Flow  Split_hour  day_of_week  is_holiday  Day  Month  feels_like  \n",
       "0        -1          22            3       False   21      1       -3.42  \n",
       "1        -1          21            3       False   28      1       -5.97  \n",
       "2        -1          16            2       False    3      2       -4.41  \n",
       "3        -1          16            3       False    4      2       -1.97  \n",
       "4        -1          15            6       False    7      2       -0.96  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('Merged_Data_202101_202303_v3.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python version RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_df[['station_id', 'Split_hour', 'day_of_week', 'is_holiday', 'feels_like', 'Day', 'Month']]\n",
    "y = data_df['Net_Flow']\n",
    "\n",
    "\n",
    "#X = pd.get_dummies(X, columns=['Split_hour', 'day_of_week', 'is_holiday', 'Day', 'Month'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 10, max_depth: 5, MSE:  7.2230, Time:  14.5742s\n",
      "n_estimators: 10, max_depth: 10, MSE:  6.5828, Time:  27.2400s\n",
      "n_estimators: 10, max_depth: 15, MSE:  6.1794, Time:  39.0072s\n",
      "n_estimators: 20, max_depth: 5, MSE:  7.2268, Time:  29.0404s\n",
      "n_estimators: 20, max_depth: 10, MSE:  6.5766, Time:  54.6136s\n",
      "n_estimators: 20, max_depth: 15, MSE:  6.1416, Time:  77.7038s\n",
      "n_estimators: 30, max_depth: 5, MSE:  7.2234, Time:  43.3882s\n",
      "n_estimators: 30, max_depth: 10, MSE:  6.5741, Time:  81.8475s\n",
      "n_estimators: 30, max_depth: 15, MSE:  6.1353, Time:  116.5622s\n",
      "Best MSE: 6.135294515412483 with n_estimators = 30 and max_depth = 15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "n_estimators_range = [10, 20, 30]  \n",
    "max_depth_range = [5, 10, 15]    \n",
    "\n",
    "best_mse = float('inf')\n",
    "best_n = 0\n",
    "best_depth = 0\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    for depth in max_depth_range:\n",
    "        start_time = time.time()\n",
    "        model = RandomForestRegressor(n_estimators=n, max_depth=depth, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        time_inter = time.time() - start_time\n",
    "        print(f\"n_estimators: {n}, max_depth: {depth}, MSE: {mse: .4f}, Time: {time_inter: .4f}s\")\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_n = n\n",
    "            best_depth = depth\n",
    "\n",
    "print(f\"Best MSE: {best_mse} with n_estimators = {best_n} and max_depth = {best_depth}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "from joblib import dump, load\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=40, max_depth=15, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "dump(model, 'random_forest_model.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict net_flows for every station in next 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id  Split_hour  day_of_week  is_holiday  feels_like  Day  Month  \\\n",
      "0           1          19            1       False         3.0   22      4   \n",
      "1           1          20            1       False         3.0   22      4   \n",
      "2           1          21            1       False         3.0   22      4   \n",
      "3           1          22            1       False         3.0   22      4   \n",
      "4           1          23            1       False         3.0   22      4   \n",
      "\n",
      "   predicted_net_flow  \n",
      "0            1.227608  \n",
      "1           -0.847306  \n",
      "2           -1.196946  \n",
      "3            0.270404  \n",
      "4            0.219386  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "next_24_hours = pd.date_range(start=pd.Timestamp('now').floor('H'), periods=24, freq='H')\n",
    "all_stations = data_df['station_id'].unique()\n",
    "\n",
    "prediction_data = pd.DataFrame({\n",
    "    'datetime': np.tile(next_24_hours, len(all_stations)),\n",
    "    'station_id': np.repeat(all_stations, len(next_24_hours))\n",
    "})\n",
    "\n",
    "prediction_data['Split_hour'] = prediction_data['datetime'].dt.hour\n",
    "prediction_data['day_of_week'] = 1\n",
    "prediction_data['is_holiday'] = False\n",
    "prediction_data['feels_like'] = 3.0 \n",
    "prediction_data['Day'] = 22\n",
    "prediction_data['Month'] = 4\n",
    "prediction_data = prediction_data[['station_id', 'Split_hour', 'day_of_week', 'is_holiday', 'feels_like', 'Day', 'Month']]\n",
    "\n",
    "y_pred = model.predict(prediction_data)\n",
    "\n",
    "prediction_data['predicted_net_flow'] = y_pred\n",
    "\n",
    "print(prediction_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data.to_csv('Predicted_data_0422.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark version RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 22:29:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 22:29:57 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomForestTraining\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.9\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- Split_hour: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_holiday: boolean (nullable = true)\n",
      " |-- feels_like: double (nullable = true)\n",
      " |-- Net_Flow: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Merged_Data_202101_202303_v3.csv', header=True, inferSchema=True)\n",
    "df = df.select(\"station_id\", \"Split_hour\", \"day_of_week\", \"is_holiday\", \"feels_like\", \"Net_Flow\", \"Day\", \"Month\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#df = df.withColumn(\"is_holiday\", col(\"is_holiday\").cast(\"string\"))\n",
    "\n",
    "# First, ensure any categorical features are indexed if they're not already\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column+\"_index\")\n",
    "    for column in [\"station_id\", \"Split_hour\", \"day_of_week\", \"Day\"]\n",
    "]\n",
    "# Now create a VectorAssembler to combine all features into one vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"station_id_index\", \"Split_hour_index\", \"day_of_week_index\", \"is_holiday\", \"Day_index\", \"feels_like\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "transformed_df = model.transform(df)\n",
    "\n",
    "(train_data, test_data) = transformed_df.randomSplit([0.7, 0.3], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 22:34:37 WARN DAGScheduler: Broadcasting large task binary with size 1353.8 KiB\n",
      "24/04/22 22:34:42 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/04/22 22:34:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/22 22:34:57 WARN DAGScheduler: Broadcasting large task binary with size 7.1 MiB\n",
      "24/04/22 22:35:08 WARN DAGScheduler: Broadcasting large task binary with size 12.3 MiB\n",
      "24/04/22 22:35:22 WARN DAGScheduler: Broadcasting large task binary with size 1402.9 KiB\n",
      "24/04/22 22:35:25 WARN DAGScheduler: Broadcasting large task binary with size 17.7 MiB\n",
      "24/04/22 22:35:43 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:35:46 WARN DAGScheduler: Broadcasting large task binary with size 16.5 MiB\n",
      "24/04/22 22:35:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:35:55 WARN DAGScheduler: Broadcasting large task binary with size 16.5 MiB\n",
      "24/04/22 22:36:04 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:36:07 WARN DAGScheduler: Broadcasting large task binary with size 16.2 MiB\n",
      "24/04/22 22:36:14 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:36:17 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n",
      "24/04/22 22:36:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:36:29 WARN DAGScheduler: Broadcasting large task binary with size 21.7 MiB\n",
      "24/04/22 22:36:37 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:36:41 WARN DAGScheduler: Broadcasting large task binary with size 18.5 MiB\n",
      "24/04/22 22:36:51 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:36:55 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n",
      "24/04/22 22:37:04 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:37:08 WARN DAGScheduler: Broadcasting large task binary with size 17.1 MiB\n",
      "24/04/22 22:37:17 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:37:21 WARN DAGScheduler: Broadcasting large task binary with size 17.3 MiB\n",
      "24/04/22 22:37:28 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:37:32 WARN DAGScheduler: Broadcasting large task binary with size 18.6 MiB\n",
      "24/04/22 22:37:41 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:37:46 WARN DAGScheduler: Broadcasting large task binary with size 15.8 MiB\n",
      "24/04/22 22:37:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:37:57 WARN DAGScheduler: Broadcasting large task binary with size 18.0 MiB\n",
      "24/04/22 22:38:11 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:38:16 WARN DAGScheduler: Broadcasting large task binary with size 14.9 MiB\n",
      "24/04/22 22:38:24 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:38:29 WARN DAGScheduler: Broadcasting large task binary with size 17.0 MiB\n",
      "24/04/22 22:38:38 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:38:43 WARN DAGScheduler: Broadcasting large task binary with size 18.0 MiB\n",
      "24/04/22 22:38:54 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:38:59 WARN DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n",
      "24/04/22 22:39:09 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:39:14 WARN DAGScheduler: Broadcasting large task binary with size 16.2 MiB\n",
      "24/04/22 22:39:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:39:29 WARN DAGScheduler: Broadcasting large task binary with size 20.9 MiB\n",
      "24/04/22 22:39:42 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:39:46 WARN DAGScheduler: Broadcasting large task binary with size 15.4 MiB\n",
      "24/04/22 22:40:00 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/04/22 22:40:04 WARN DAGScheduler: Broadcasting large task binary with size 17.7 MiB\n",
      "24/04/22 22:40:13 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:40:17 WARN DAGScheduler: Broadcasting large task binary with size 23.6 MiB\n",
      "24/04/22 22:40:24 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:40:28 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n",
      "24/04/22 22:40:40 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:40:45 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "24/04/22 22:40:54 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:40:58 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
      "24/04/22 22:41:07 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/04/22 22:41:12 WARN DAGScheduler: Broadcasting large task binary with size 14.1 MiB\n",
      "24/04/22 22:41:18 WARN DAGScheduler: Broadcasting large task binary with size 1599.2 KiB\n",
      "24/04/22 22:41:21 WARN DAGScheduler: Broadcasting large task binary with size 11.7 MiB\n",
      "24/04/22 22:41:24 WARN DAGScheduler: Broadcasting large task binary with size 1804.7 KiB\n",
      "24/04/22 22:41:27 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "24/04/22 22:41:32 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "24/04/22 22:41:36 WARN DAGScheduler: Broadcasting large task binary with size 1081.4 KiB\n",
      "24/04/22 22:41:39 WARN TaskSetManager: Stage 109 contains a task of very large size (18824 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_mse = RegressionEvaluator(labelCol=\"Net_Flow\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(columns=[\"numTrees\", \"maxDepth\", \"MSE\", \"Time\"])\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Net_Flow\", numTrees=30, maxDepth=15, maxBins=1000, seed=42)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_model.write().overwrite().save(\"Spark_N_30_D_15\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import time\n",
    "numTrees_list = [10, 20, 30]\n",
    "maxDepth_list = [5, 10, 15]\n",
    "\n",
    "evaluator_mse = RegressionEvaluator(labelCol=\"Net_Flow\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(columns=[\"numTrees\", \"maxDepth\", \"MSE\", \"Time\"])\n",
    "\n",
    "for num in numTrees_list:\n",
    "    for depth in maxDepth_list:\n",
    "        start_time = time.time()\n",
    "        rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Net_Flow\", numTrees=num, maxDepth=depth, maxBins=1000, seed=42)\n",
    "        rf_model = rf.fit(train_data)\n",
    "        \n",
    "        predictions = rf_model.transform(test_data)\n",
    "        \n",
    "        mse = evaluator_mse.evaluate(predictions)\n",
    "        time_inter = time.time() - start_time\n",
    "        print(f\"numTrees: {num}, maxDepth: {depth}, MSE: {mse: .4f}, Time: {time_inter: .4f}s\")\n",
    "        new_row = pd.DataFrame({\"numTrees\": [num], \"maxDepth\": [depth], \"MSE\": [mse], \"Time\": [time_inter]})\n",
    "        results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numTrees</th>\n",
       "      <th>maxDepth</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>6.908905</td>\n",
       "      <td>23.803790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6.159883</td>\n",
       "      <td>34.212571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>5.934817</td>\n",
       "      <td>104.680240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.894684</td>\n",
       "      <td>26.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>6.144721</td>\n",
       "      <td>56.503823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>5.867577</td>\n",
       "      <td>239.560308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>6.854832</td>\n",
       "      <td>103.332939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  numTrees maxDepth       MSE        Time\n",
       "0       10        5  6.908905   23.803790\n",
       "1       10       10  6.159883   34.212571\n",
       "2       10       15  5.934817  104.680240\n",
       "3       20        5  6.894684   26.530400\n",
       "4       20       10  6.144721   56.503823\n",
       "5       20       15  5.867577  239.560308\n",
       "6      100        5  6.854832  103.332939"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numTrees</th>\n",
       "      <th>maxDepth</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>6.861225</td>\n",
       "      <td>30.473653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>6.112688</td>\n",
       "      <td>92.362313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>5.847989</td>\n",
       "      <td>420.506886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  numTrees maxDepth       MSE        Time\n",
       "0       30        5  6.861225   30.473653\n",
       "1       30       10  6.112688   92.362313\n",
       "2       30       15  5.847989  420.506886"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark without weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   numTrees maxDepth       MSE\n",
      "0         5        3  7.155212\n",
      "1         5        5  6.779841\n",
      "2         5       10  6.269571\n",
      "3         5       15  6.215765\n",
      "4        10        3  7.148010\n",
      "5        10        5  6.866187\n",
      "6        10       10  6.261236\n",
      "7        10       15  6.197606\n",
      "8        20        3  7.095159\n",
      "9        20        5  6.787787\n",
      "10       20       10  6.257980\n",
      "11       20       15  6.187825\n"
     ]
    }
   ],
   "source": [
    "print(results)##without weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
